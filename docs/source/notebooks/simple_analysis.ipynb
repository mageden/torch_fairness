{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier on a simple synthetic dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bunch of text - I hope it looks normal! (make stuff hidden)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will demonstrate how to train a classifier with and without fairness regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Required for notebook path to be at head of project for torch_fairness imports\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../..'))\n",
    "import os\n",
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch_fairness.data import SensitiveMap\n",
    "from torch_fairness.data import SensitiveTransformer\n",
    "from torch_fairness.metrics import AccuracyEquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features: torch.tensor, labels: torch.tensor, sensitive: torch.tensor, threshold = 0.5):\n",
    "    fairness_measure = AccuracyEquality(sensitive_map=sensitive_map, threshold=threshold)\n",
    "    metrics = {\n",
    "        'accuracy': torch.mean(1.0*((1.0*(model(features)>threshold))==labels)).item(),\n",
    "        'fairness': fairness_measure(pred=model(features), sensitive=sensitive, labels=labels).detach().numpy()\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this example is a simple synthetic dataset that was generated with a single binary sensitive attribute (e.g., minority, majority). Additionally, it was generated with class imbalance (majority >> minority) and measurement invariance (relationship between features and labels differs between groups). These two attributes ensure that a model that is trained without any form of resampling, loss reweighting, or regularization will produce predictions that are more accurate for the majority vs. minority group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_cols = ['demo']\n",
    "label_col = ['hired']\n",
    "features_cols = ['X_0', 'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9']\n",
    "data = pd.read_csv(os.path.join('datasets', 'synthetic_binary.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to specify the sensitive attributes using a SensitiveMap. This object is used to keep to keep track of the sensitive groups, produce dummy coded versions of the sensitive variables, and match minority-majority groups in fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_map = SensitiveMap(\n",
    "    {'name': 'demo', 'majority': 'majority', 'minority': ['minority']}, \n",
    ")\n",
    "sensitive_transformer = SensitiveTransformer(sensitive_map=sensitive_map)\n",
    "sensitive_transformer.fit(data[sensitive_cols])\n",
    "dummy_sensitive = sensitive_transformer.transform(data[sensitive_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_s, test_s, train_y, test_y = train_test_split(\n",
    "    torch.tensor(data[features_cols].values, dtype=torch.float32),\n",
    "    dummy_sensitive, \n",
    "    torch.tensor(data[label_col].values, dtype=torch.float32), \n",
    "    shuffle=True, \n",
    "    train_size=0.7,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model without fairness objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model will be our baseline and will not include any strategy to address the class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 4818.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(len(features_cols), 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs.squeeze(), train_y.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'accuracy': 0.9371428489685059, 'fairness': array([0.13965851], dtype=float32)}\n",
      "Test: {'accuracy': 0.8933333158493042, 'fairness': array([0.14627284], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "train_metrics = evaluate(model, features=train_x, labels=train_y, sensitive=train_s)\n",
    "test_metrics = evaluate(model, features=test_x, labels=test_y, sensitive=test_s)\n",
    "print(f'Training: {train_metrics}')\n",
    "print(f'Test: {test_metrics}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with fairness objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second model, we will include a fairness measure, AccuracyEquality, which uses the difference in accuracy between the minority-majority group as a regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1265.81it/s]\n"
     ]
    }
   ],
   "source": [
    "fair_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(len(features_cols), 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "fairness_measure = AccuracyEquality(sensitive_map=sensitive_map)\n",
    "criterion = torch.nn.BCELoss()\n",
    "fairness_measure = AccuracyEquality(sensitive_map=sensitive_map)\n",
    "optimizer = torch.optim.Adam(fair_model.parameters(), lr=0.1)\n",
    "for epoch in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = fair_model(train_x)\n",
    "    crit_loss = criterion(outputs.squeeze(), train_y.squeeze())\n",
    "    fair_loss = fairness_measure(pred=outputs, sensitive=train_s, labels=train_y)\n",
    "    loss = crit_loss + fair_loss.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'accuracy': 0.8999999761581421, 'fairness': array([0.00272262], dtype=float32)}\n",
      "Test: {'accuracy': 0.8500000238418579, 'fairness': array([0.05123568], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "train_metrics = evaluate(fair_model, features=train_x, labels=train_y, sensitive=train_s)\n",
    "test_metrics = evaluate(fair_model, features=test_x, labels=test_y, sensitive=test_s)\n",
    "print(f'Training: {train_metrics}')\n",
    "print(f'Test: {test_metrics}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we observed a decrease in over-all accuracy but an increase in fairness for the accuracy disparity. The degree of trade-off can be controlled through modifying the weighting between the criterion and fairness loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial hopefully provided a simple example of how a fairness regularized model can be trained and evaluated using this package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f196a898649953439d4e1489d12a148acaa256d4f91c3ce1913e9e7c137c2b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
